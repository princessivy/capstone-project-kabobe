{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing_methods_NEW.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pds2122/capstone-project-kabobe/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install, import & download"
      ],
      "metadata": {
        "id": "YZJ2L6cEoNn8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IIEv3AMFwx__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6331b18-f48a-4e50-ba62-dabf8ffcd538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5 MB 18.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 981 kB 25.5 MB/s \n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install ndjson --quiet\n",
        "!pip install beautifulsoup4 --quiet\n",
        "!pip install html2text --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install HanTa --quiet\n",
        "!pip install langdetect --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import gzip\n",
        "import json\n",
        "import nltk\n",
        "import ndjson\n",
        "import requests\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from langdetect import detect\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "from collections import OrderedDict, Counter\n",
        "from HanTa import HanoverTagger as ht\n",
        "from urllib.parse import urlsplit, urlunsplit"
      ],
      "metadata": {
        "id": "a4hUHNhYw7RL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download additional stopwords and wordnet to use for German\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjttbJA6BLWL",
        "outputId": "20246523-19d1-4cef-ed5a-80aeaaa39d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Datasets"
      ],
      "metadata": {
        "id": "NxXUZYi303FW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/gdrive')\n",
        "# Define file paths\n",
        "data_path = Path('/gdrive/MyDrive/industry_data/')\n",
        "test_file = 'test_small.ndjson.gz'\n",
        "train_file = 'train_small.ndjson.gz'"
      ],
      "metadata": {
        "id": "M9dDlWvvxEPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0625b06c-0e85-43a6-e7b6-4cedf8360166"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gzip.open(data_path/train_file, \"rt\", encoding='UTF-8') as file:\n",
        "    data = []\n",
        "    data = [json.loads(line) for line in file]\n",
        "\n",
        "# get nested list, create flat-list, to fet data in DataFrame\n",
        "flat_list = [item for sublist in data for item in sublist]\n",
        "df_train = pd.DataFrame(flat_list)"
      ],
      "metadata": {
        "id": "-na9FD0mxJN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gzip.open(data_path/test_file, \"rt\", encoding='UTF-8') as file:\n",
        "    data = []\n",
        "    data = [json.loads(line) for line in file]\n",
        "    df_test = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "j18Jesf2xIrU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading as function - used in the mean time of preprocessing in order to continue \n",
        "def data_reloader_from_zip(file_name):\n",
        "  with gzip.open(file_name, 'rt', encoding='UTF-8') as file:\n",
        "      data = []\n",
        "      data = [ndjson.loads(line.strip()) for line in file]\n",
        "\n",
        "  flat_list = [item for sublist in data for item in sublist]\n",
        "  df = pd.DataFrame(flat_list)\n",
        "\n",
        "  return df\n",
        "\n",
        "def data_reloader_from_ndjson(file_name):\n",
        "  with open(file_name, 'rt', encoding='UTF-8') as file:\n",
        "      data = []\n",
        "      data = [ndjson.loads(line.strip()) for line in file]\n",
        "\n",
        "  flat_list = [item for sublist in data for item in sublist]\n",
        "  df = pd.DataFrame(flat_list)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "CsONWVSgokXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Datasets"
      ],
      "metadata": {
        "id": "SfTTIgjvoc6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save to ndjson (either regular or gzip)\n",
        "\n",
        "def datasaver_to_zip(df, name):\n",
        "  # create flat list in dict form: {'col1': 'value', 'col2': 'value', ...} from df\n",
        "  flat_list_back = []\n",
        "  for i in range(len(df)):\n",
        "    line = df.loc[i].to_dict()\n",
        "    #line['industry'] = str(line['industry']) # use if idustry number (e.g. 13) should be enclosed in '' (e.g. '13')\n",
        "    flat_list_back.append([line])\n",
        "\n",
        "  filename_zip = str(name) + '.ndjson.gz'\n",
        "\n",
        "  with gzip.open(filename_zip, 'wt', encoding='UTF-8') as z:\n",
        "    for item in flat_list_back:\n",
        "      z.write('{}\\n'.format(ndjson.dumps(item)))\n",
        "\n",
        "\n",
        "def datasaver_to_ndjson(df, name):\n",
        "  # create flat list in dict form: {'col1': 'value', 'col2': 'value', ...} from df\n",
        "  flat_list_back = []\n",
        "  for i in range(len(df)):\n",
        "    line = df.loc[i].to_dict()\n",
        "    #line['industry'] = str(line['industry']) # use if idustry number (e.g. 13) should be enclosed in '' (e.g. '13')\n",
        "    flat_list_back.append([line])\n",
        "\n",
        "  filename = str(name) + '.ndjson'\n",
        "\n",
        "  # https://stackoverflow.com/questions/21058935/python-json-loads-shows-valueerror-extra-data\n",
        "  with open(filename, mode='w') as f:\n",
        "    for item in flat_list_back:\n",
        "      f.write('{}\\n'.format(ndjson.dumps(item))) "
      ],
      "metadata": {
        "id": "qZ8q_Qq8ocvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse Dataset\n",
        "\n",
        "Look at occurences of tags"
      ],
      "metadata": {
        "id": "Dez9GJCrospf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tags = [] \n",
        "\n",
        "# of the first 1000 rows - alternatively choose len(df_train)\n",
        "for i in range(1000): #len(df_train)\n",
        "  soup = BeautifulSoup(data[i][0]['html'], 'html.parser')\n",
        "  #for tag in soup.findAll(True):\n",
        "    #print(tag.name)\n",
        "  tags = set(tag.name for tag in BeautifulSoup(data[i][0]['html'], 'html.parser').find_all()) # if eliminating set, you get the sum of all occurences\n",
        "  all_tags.extend(tags)"
      ],
      "metadata": {
        "id": "9avr5ZCPow7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counted = Counter(all_tags)\n",
        "OrderedDict(counted.most_common())"
      ],
      "metadata": {
        "id": "IkAwPEzZozNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods for Preprocessing"
      ],
      "metadata": {
        "id": "AWirKgRCxSKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pure_text(soup):\n",
        "    return clean_text(soup.text)\n",
        "\n",
        "\n",
        "def get_lang_code(pure_text):\n",
        "    lang_code = ''\n",
        "    try:\n",
        "        lang_code = detect(pure_text)\n",
        "    except:\n",
        "        lang_code = 'NaN'\n",
        "    return lang_code\n",
        "\n",
        "\n",
        "def get_img_alt(soup):\n",
        "    img_alt = ''\n",
        "    retrieved_imgs = soup.findAll('img', alt = True)\n",
        "    for i in range(len(retrieved_imgs)):\n",
        "        alt = retrieved_imgs[i]\n",
        "        img_alt = img_alt + ' ' + alt['alt']\n",
        "    return img_alt\n",
        "\n",
        "\n",
        "def concatenate_columns(df):\n",
        "  df['concatenated'] = df[df.columns[3:]].apply(\n",
        "      lambda column: ' '.join(column.dropna().astype(str)),\n",
        "      axis=1\n",
        "  )\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_sentiment(pure_text):\n",
        "  return round(TextBlob(pure_text).sentiment.polarity,2)"
      ],
      "metadata": {
        "id": "qxRWA2aNygVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the preprocessing we noticed multiple stopwords which have not been removed by the standard stopwords. \n",
        "<br /> Therefore we customized them:"
      ],
      "metadata": {
        "id": "PLDCfmJ5xi_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXTENTION_STOPWORDS = [\n",
        "    'facebook', 'w', 'm', 'd', 'instagram', 'youtube', 'xing', 'linkedin', \n",
        "    'twitter', 'snapchat', 'mehr', 'dafür', 'beim', 'davon', 'somit'\n",
        "]"
      ],
      "metadata": {
        "id": "gnb6Q4CRxfNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    if text == 'nan':\n",
        "      return\n",
        "    \n",
        "    # https://medium.com/analytics-vidhya/applying-text-classification-using-logistic-regression-a-comparison-between-bow-and-tf-idf-1f1ed1b83640\n",
        "    # convert words to lower case\n",
        "    content = text.lower()\n",
        "    \n",
        "    content = re.sub(r'&amp;', '', content) \n",
        "    content = re.sub(r'[_\"\\-;%()–|„”®+&=¤*%.™,“!’€?:#$@\\[\\]/]', ' ', content)\n",
        "    content = re.sub(r'<br />', ' ', content)\n",
        "    content = re.sub(r'\\'', ' ', content)\n",
        "    content = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+${}\", \" \", content)\n",
        "    content = re.sub(r'[0-9]', ' ', content)\n",
        "    content = content.replace('{', '')\n",
        "    content = content.replace('}', '')\n",
        "    \n",
        "    \n",
        "    # remove stopwords\n",
        "    content = content.split()\n",
        "    stops = stopwords.words('german')\n",
        "    # Extend standard stopwords with custom.\n",
        "    stops.extend(EXTENTION_STOPWORDS)\n",
        "    stops = set(stops)\n",
        "    content = [w for w in content if not w in stops]\n",
        "    content = ' '.join(content)\n",
        "\n",
        "    # tokenize each word\n",
        "    content =  nltk.WordPunctTokenizer().tokenize(content)\n",
        "    \n",
        "    # lemmatize each token in German (reduce words to stem)\n",
        "    tagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
        "    word_list = []\n",
        "    for w in content:\n",
        "        lemma = [lemma for (word,lemma,pos) in tagger.tag_sent(w.split())]\n",
        "        word_list.append(' '.join(lemma))\n",
        "    return str.lower(' '.join(word_list))"
      ],
      "metadata": {
        "id": "hFsZMwk2xhwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features_from_html(df, feature_list):\n",
        "  \n",
        "  # Add features as new empty columns\n",
        "  df = pd.concat([df,pd.DataFrame(columns=feature_list)]) #df.reindex(columns=list('ABCD'), fill_value=0)\n",
        "  for row in range(len(df)):\n",
        "    soup = BeautifulSoup(df.html[row])\n",
        "    for feature in feature_list:\n",
        "        retrieved_features = soup.findAll(feature)\n",
        "        for i in range(len(retrieved_features)):\n",
        "            if i == 0:\n",
        "                df[feature][row] = retrieved_features[i].text\n",
        "            else:\n",
        "                df[feature][row] = df[feature][row] + ' ' + retrieved_features[i].text\n",
        "    if 'pure_text' in feature_list:\n",
        "      df['pure_text'][row] = get_pure_text(soup)\n",
        "    if 'lang_code' in feature_list:\n",
        "      df['lang_code'][row] = get_lang_code(df['pure_text'][row])\n",
        "    if 'img_alt' in feature_list:\n",
        "      df['img_alt'][row] = get_img_alt(soup)\n",
        "    if 'sentiment_analysis' in feature_list:\n",
        "      df['sentiment_analysis'][row] = get_sentiment(df['pure_text'][row])\n",
        "  return df"
      ],
      "metadata": {
        "id": "Cy1xqSkMyBN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# union of all processes so that only one function can be called\n",
        "def execute_preprocessing(df, feature_list):\n",
        "\n",
        "  no_pre_cols = ['url', 'html', 'industry', 'industry_label', 'sentiment_analysis']\n",
        "  df = get_features_from_html(df, feature_list)\n",
        "\n",
        "  # clean features\n",
        "  for column in df:\n",
        "    if column not in no_pre_cols:\n",
        "      for i in range(len(df)):\n",
        "        try:\n",
        "          df[column][i] = clean_text(df[column][i])\n",
        "        except:\n",
        "          print(df[column][i])\n",
        "  df = concatenate_columns(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "s3GMcUUKDBdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution"
      ],
      "metadata": {
        "id": "QxuhBEFd9_C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_list = ['title', 'h1', 'h2', 'h3', 'figcaption', 'pure_text', 'img_alt', 'strong', 'bold', 'lang_code', 'sentiment_analysis']\n",
        "df_preprocessed = execute_preprocessing(df_train, feature_list)\n",
        "df_preprocessed.head()"
      ],
      "metadata": {
        "id": "s6M1M0KcpV6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Archive\n",
        "\n",
        "### With the following code, we prepared the train- and test-data. we decided to make this code a little nicer. see above"
      ],
      "metadata": {
        "id": "aAMyZO3vmufs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "get the whole text between tags"
      ],
      "metadata": {
        "id": "IWdYSdGenLpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_to_text(html):\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "    # kill all script and style elements\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()    # rip it out\n",
        "\n",
        "    # get text\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    # break multi-headlines into a line each\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    # drop blank lines\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "0wuf4-v3nP2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eliminate html elements from text, return text elements\n",
        "# 21 minutes for train dataset\n",
        "\n",
        "# assign column for new text\n",
        "df_train['html_to_text'] = ''\n",
        "\n",
        "for line in range(0, len(df_train)):\n",
        "  content = parse_to_text(df_train.html[line])\n",
        "  df_train.html_to_text[line] = content\n",
        "\n",
        "\n",
        "# on test dataset\n",
        "# 7 minutes\n",
        "\n",
        "df_test['html_to_text'] = ''\n",
        "\n",
        "for line in range(0, len(df_test)):\n",
        "  content = parse_to_text(df_test.html[line])\n",
        "  df_test.html_to_text[line] = content"
      ],
      "metadata": {
        "id": "YOifnq-lnrjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"clean text\" method is the same!**"
      ],
      "metadata": {
        "id": "3pXl8TvknzBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "specific tags and features"
      ],
      "metadata": {
        "id": "3rfVhwqMnDBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getHTML(url):\n",
        "    return  BeautifulSoup(url, 'html.parser')\n",
        "\n",
        "## Img-Description from IMG-Tag\n",
        "def getImgDescriptionHTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    results = soup.find_all('img', alt = True)\n",
        "    img_description = []\n",
        "    for x in range(0,len(results)):\n",
        "      first_result = results[x]\n",
        "      img_description.append(first_result['alt'])\n",
        "    \n",
        "    return list(filter(None, img_description))\n",
        "\n",
        "## Title\n",
        "def getTitleHTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    if (soup.title is not None):\n",
        "        return str(soup.title.string)\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "## h1\n",
        "def getH1HTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('h1')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))\n",
        "\n",
        "## h2\n",
        "def getH2HTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('h2')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))\n",
        "\n",
        "## h3\n",
        "def getH3HTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('h3')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))\n",
        "\n",
        "## strong - fragwürdig\n",
        "def getStrongHTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('strong')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))\n",
        "\n",
        "## bold\n",
        "def getBoldHTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('bold')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))\n",
        "\n",
        "## language code\n",
        "def getLangHTMLtag(url):\n",
        "    \n",
        "    try:\n",
        "      soup = getHTML(url)\n",
        "      body_text = soup.body.get_text()\n",
        "      return detect(body_text)\n",
        "    \n",
        "    except:\n",
        "      return str(\"NaN\")\n",
        "    \n",
        "## figcaption\n",
        "def getFigCaptionHTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('figcaption')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))"
      ],
      "metadata": {
        "id": "4VI7Q4k0mvdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill extra features\n",
        "\n",
        "def retrieve_features(df):\n",
        "    for i in range (4, 13):\n",
        "        for j in range(0, len(df)):\n",
        "            if i == 4:\n",
        "              df.iloc[:, i][j] = getImgDescriptionHTMLtag(df.html[j])\n",
        "            elif i == 5:\n",
        "              df.iloc[:, i][j] = getTitleHTMLtag(df.html[j])\n",
        "            elif i == 6:\n",
        "              df.iloc[:, i][j] = getH1HTMLtag(df.html[j])\n",
        "            elif i == 7:\n",
        "              df.iloc[:, i][j] = getH2HTMLtag(df.html[j])\n",
        "            elif i == 8:\n",
        "              df.iloc[:, i][j] = getH3HTMLtag(df.html[j])\n",
        "            elif i == 9:\n",
        "              df.iloc[:, i][j] = getStrongHTMLtag(df.html[j])\n",
        "            elif i == 10:\n",
        "              df.iloc[:, i][j] = getBoldHTMLtag(df.html[j])\n",
        "            elif i == 11:\n",
        "              df.iloc[:, i][j] = getLangHTMLtag(df.html[j])\n",
        "            elif i == 12:\n",
        "              df.iloc[:, i][j] = getFigCaptionHTMLtag(df.html[j])"
      ],
      "metadata": {
        "id": "Eo3LY6b3n5Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert list elements to string, in order to clean text in each column\n",
        "def convert_features_toString(df):\n",
        "\n",
        "    for x in range(len(df)):\n",
        "        df.img_alt[x] = ' '.join(df.img_alt[x])\n",
        "        df.title[x] = ' '.join(df.title[x])\n",
        "        df.h1[x] = ' '.join(df.h1[x])\n",
        "        df.h2[x] = ' '.join(df.h2[x])\n",
        "        df.h3[x] = ' '.join(df.h3[x])\n",
        "        df.strong[x] = ' '.join(df.strong[x])\n",
        "        df.bold[x] = ' '.join(df.bold[x])\n",
        "        df.figcaption[x] = ' '.join(df.figcaption[x])"
      ],
      "metadata": {
        "id": "u2ZCYVtXn59n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_features_toString_old(df):\n",
        "    # img\n",
        "    for x in range(len(df)):\n",
        "        df.img_alt[x] = ' '.join(df.img_alt[x])\n",
        "\n",
        "    # title\n",
        "    for x in range(len(df)):\n",
        "        df.title[x] = ' '.join(df.title[x])\n",
        "\n",
        "    # h1\n",
        "    for x in range(len(df)):\n",
        "        df.h1[x] = ' '.join(df.h1[x])\n",
        "\n",
        "    # h2\n",
        "    for x in range(len(df)):\n",
        "        df.h2[x] = ' '.join(df.h2[x])\n",
        "\n",
        "    # h3\n",
        "    for x in range(len(df)):\n",
        "        df.h3[x] = ' '.join(df.h3[x])\n",
        "\n",
        "    # strong\n",
        "    for x in range(len(df)):\n",
        "        df.strong[x] = ' '.join(df.strong[x])\n",
        "\n",
        "    # bold\n",
        "    for x in range(len(df)):\n",
        "        df.bold[x] = ' '.join(df.bold[x])\n",
        "\n",
        "    # figcaption\n",
        "    for x in range(len(df)):\n",
        "        df.figcaption[x] = ' '.join(df.figcaption[x])\n",
        "\n",
        "## Welche nehmen wir hier? Irgendwie funktioniert das nur, wenn ich es außerhalb der Funktion mache - nur bei mir so? wieso?"
      ],
      "metadata": {
        "id": "DM4OVXi-n8A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alle extra-features durch clean_text schicken, ohne lang-feature\n",
        "def clean_dataframe(df):\n",
        "    columns = ['img_alt','title','h1','h2','h3','strong','bold','figcaption']\n",
        "    for x in columns:\n",
        "        for y in range(len(df)):\n",
        "            df[x][y] = clean_text(df[x][y])"
      ],
      "metadata": {
        "id": "_KSk1ew8n-Yf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}