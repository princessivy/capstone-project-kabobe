{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing_methods_NEW.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pds2122/capstone-project-kabobe/blob/main/preprocessing_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains the identical preprocessing steps as \"preprocessing\". Here, only the mothods are saved. No data import or execution, as the notebook has to be run before importing."
      ],
      "metadata": {
        "id": "D1yQR-_2_c8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install, import & download "
      ],
      "metadata": {
        "id": "gn3vYVJRnLCO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIEv3AMFwx__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa925cd2-e4f2-46af-8238-bb1fc82b0c76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 97 kB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 26.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 981 kB 25.1 MB/s \n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install ndjson --quiet\n",
        "!pip install beautifulsoup4 --upgrade --quiet\n",
        "!pip install html2text --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install HanTa --quiet\n",
        "!pip install langdetect --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import gzip\n",
        "import json\n",
        "import nltk\n",
        "import ndjson\n",
        "import requests\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from langdetect import detect\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "from HanTa import HanoverTagger as ht\n",
        "from urllib.parse import urlsplit, urlunsplit"
      ],
      "metadata": {
        "id": "a4hUHNhYw7RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oXur_BWUv9j",
        "outputId": "7510730e-68aa-43c2-d964-3c2e2dcbb40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods for Preprocessing"
      ],
      "metadata": {
        "id": "AWirKgRCxSKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_source_code(url):\n",
        "    # route to starting page of website\n",
        "    split_url = urlsplit(url)\n",
        "    clean_path = \"\".join(split_url.scheme+\"://\"+split_url.netloc+\"/\")\n",
        "\n",
        "    # get html content\n",
        "    url = str(clean_path)\n",
        "    r = requests.get(url)\n",
        "    return r.text\n",
        "\n",
        "\n",
        "def get_pure_text(soup):\n",
        "    return clean_text(soup.text)\n",
        "\n",
        "\n",
        "def get_lang_code(pure_text):\n",
        "    lang_code = ''\n",
        "    try:\n",
        "        lang_code = detect(pure_text)\n",
        "    except:\n",
        "        lang_code = 'NaN'\n",
        "    return lang_code\n",
        "\n",
        "\n",
        "def get_img_alt(soup):\n",
        "    img_alt = ''\n",
        "    retrieved_imgs = soup.findAll('img', alt = True)\n",
        "    for i in range(len(retrieved_imgs)):\n",
        "        alt = retrieved_imgs[i]\n",
        "        img_alt = img_alt + ' ' + alt['alt']\n",
        "    return img_alt\n",
        "\n",
        "\n",
        "def concatenate_columns(df):\n",
        "  df['concatenated'] = df[df.columns[:]].apply(\n",
        "      lambda column: ' '.join(column.dropna().astype(str)),\n",
        "      axis=1\n",
        "  )\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_sentiment(pure_text):\n",
        "  return round(TextBlob(pure_text).sentiment.polarity,2)"
      ],
      "metadata": {
        "id": "CRTjknUBxaEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop word customizing\n",
        "EXTENTION_STOPWORDS = [\n",
        "    'facebook', 'w', 'm', 'd', 'instagram', 'youtube', 'xing', 'linkedin', \n",
        "    'twitter', 'snapchat', 'mehr', 'dafür', 'beim', 'davon', 'somit'\n",
        "]"
      ],
      "metadata": {
        "id": "gnb6Q4CRxfNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \n",
        "    # convert words to lower case\n",
        "    content = text.lower()\n",
        "    \n",
        "    content = re.sub(r'&amp;', '', content) \n",
        "    content = re.sub(r'[_\"\\-;%()–|„”®+&=¤*%.™,“<>©!’€?:#$@\\[\\]/]', ' ', content)\n",
        "    content = re.sub(r'<br />', ' ', content)\n",
        "    content = re.sub(r'\\'', ' ', content)\n",
        "    content = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", content)\n",
        "    content = re.sub(r'[0-9]', ' ', content)\n",
        "    content= content.replace('{', '')\n",
        "    content= content.replace('}', '')\n",
        "    \n",
        "    # remove stopwords\n",
        "    content = content.split()\n",
        "    stops = stopwords.words('german')\n",
        "    # Extend standard stopwords with custom.\n",
        "    stops.extend(EXTENTION_STOPWORDS)\n",
        "    stops = set(stops)\n",
        "    content = [w for w in content if not w in stops]\n",
        "    content = ' '.join(content)\n",
        "\n",
        "    # tokenize each word\n",
        "    content =  nltk.WordPunctTokenizer().tokenize(content)\n",
        "    \n",
        "    # lemmatize each token in German (reduce words to stem)\n",
        "    tagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
        "    word_list = []\n",
        "    for w in content:\n",
        "        lemma = [lemma for (word,lemma,pos) in tagger.tag_sent(w.split())]\n",
        "        word_list.append(' '.join(lemma))\n",
        "    return str.lower(' '.join(word_list))"
      ],
      "metadata": {
        "id": "hFsZMwk2xhwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features_from_url(url, feature_list):\n",
        "    page_source_code = get_page_source_code(url)\n",
        "    soup = BeautifulSoup(''.join(page_source_code))\n",
        "    feature_dict = {}\n",
        "    for feature in feature_list:\n",
        "        retrieved_features = soup.findAll(feature)\n",
        "        for i in range(len(retrieved_features)):\n",
        "            if i == 0:\n",
        "                feature_dict[feature] = retrieved_features[i].text\n",
        "            else:\n",
        "                feature_dict[feature] = feature_dict[feature] + ' ' + retrieved_features[i].text\n",
        "    if 'pure_text' in feature_list:\n",
        "        feature_dict['pure_text'] = get_pure_text(soup)\n",
        "    if 'lang_code' in feature_list:\n",
        "        feature_dict['lang_code'] = get_lang_code(feature_dict['pure_text'])\n",
        "    if 'img_alt' in feature_list:\n",
        "        feature_dict['img_alt'] = get_img_alt(soup)\n",
        "    if 'sentiment_analysis' in feature_list:\n",
        "        feature_dict['sentiment_analysis'] = get_sentiment(feature_dict['pure_text'])\n",
        "    return feature_dict"
      ],
      "metadata": {
        "id": "Cy1xqSkMyBN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_preprocessing(url, feature_list):\n",
        "  no_pre_cols = ['sentiment_analysis']\n",
        "  df = pd.DataFrame(get_features_from_url(url, feature_list), index=[0])\n",
        "  # clean features\n",
        "  for column in df:\n",
        "    if column not in no_pre_cols:\n",
        "      for i in range(len(df)):\n",
        "        try:\n",
        "          df[column][i] = clean_text(df[column][i])\n",
        "        except:\n",
        "          print(df[column][i])\n",
        "  df = concatenate_columns(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "JgKq5ASsTZ-_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}